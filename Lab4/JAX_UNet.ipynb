{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsRPo5ZE8fzx"
      },
      "source": [
        "## U-Net for Image Segmentation with JAX\n",
        "\n",
        "This Colab notebook guides you through building and training a U-Net architecture using JAX for image segmentation tasks, emphasizing the use of `jax.jit` for performance optimization.\n",
        "\n",
        "**Understanding U-Net Architecture**\n",
        "\n",
        "U-Net is a convolutional neural network architecture specifically designed for image segmentation. Its unique U-shaped structure allows it to excel at capturing both local details (fine-grained features) and contextual information  (the broader picture) within an image.  Here's how it works:\n",
        "\n",
        "* **Contracting Path (Encoder):**\n",
        "    * Consists of multiple down-convolution blocks. Each block applies a series of convolutional layers followed by an activation function (e.g., ReLU).\n",
        "    * Down-convolution layers, often with pooling operations (like max pooling), progressively reduce the spatial resolution of the input image while increasing the channels (feature maps). These extracted features become increasingly complex and high-level.\n",
        "\n",
        "* **Expansive Path (Decoder):**\n",
        "    * Composed of up-convolution blocks that utilize transposed convolutions to increase spatial resolution while decreasing channel depth.\n",
        "    * Each block upsamples the feature maps, combining them with skip connections from the corresponding level in the contracting path.\n",
        "    * Skip connections concatenate higher-resolution feature maps from the encoder with the expanded feature maps from the decoder. This helps recover precise localization details lost during downsampling.\n",
        "\n",
        "* **Output Layer:**\n",
        "    * A final convolutional layer, often with a sigmoid activation function, generates the segmentation output(s). This output typically  has the same dimensions as the input image.\n",
        "\n",
        "**The benefits of this architecture make U-Net highly effective for various image segmentation tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PT5R9O-8ply"
      },
      "source": [
        "**1. Setting Up and Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wJVe5X7N3hTS"
      },
      "outputs": [],
      "source": [
        "! pip install jax jaxlib sklearn opencv-python matplotlib flax -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "8fSr1GxO8fz0"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "\n",
        "# Additional libraries you might need based on your dataset (e.g., for loading and preprocessing)\n",
        "# ...\n",
        "import haiku as hk\n",
        "import jax.nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "from flax import linen as nn\n",
        "from typing import Callable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjF3TvHT8fz1"
      },
      "source": [
        "**2. Defining the U-Net Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "id": "ShEgtoH18fz1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @jax.jit\n",
        "def conv_layer(inputs, filters, kernel_size, padding='SAME'):\n",
        "  \"\"\"\n",
        "  Defines a single convolutional layer with ReLU activation and batch normalization.\n",
        "\n",
        "  Args:\n",
        "    inputs: Input tensor (e.g., image data).\n",
        "    filters: Number of filters in the convolutional layer.\n",
        "    kernel_size: Size of the convolutional kernel.\n",
        "    padding: Padding strategy for the convolution ('same' or 'valid').\n",
        "\n",
        "  Returns:\n",
        "    Output tensor after applying convolution, ReLU activation, and batch normalization.\n",
        "  \"\"\"\n",
        "  # Apply convolution\n",
        "  conv = nn.Conv(features=filters, kernel_size=(kernel_size, kernel_size),padding=padding)(inputs)\n",
        "\n",
        "  # Apply ReLU activation (optional)\n",
        "  activated = nn.relu(conv)\n",
        "\n",
        "  # Apply batch normalization (optional)\n",
        "  normalized = nn.BatchNorm(use_running_average=True, momentum=0.999, epsilon=1e-5)(activated)\n",
        "\n",
        "  return normalized\n",
        "\n",
        "# @jax.jit\n",
        "def conv_block(inputs, filters, kernel_size):\n",
        "  \"\"\"Defines a convolutional block with activation and normalization (jitted).\"\"\"\n",
        "  # ... implement convolutional layers, ReLU activation, and batch normalization\n",
        "\n",
        "  conv1 = conv_layer(inputs, filters, kernel_size)\n",
        "  print(f\"conv1{conv1.shape}\")\n",
        "  conv2 = conv_layer(conv1, filters, kernel_size)\n",
        "  print(f\"conv2{conv2.shape}\")\n",
        "  output = conv2\n",
        "\n",
        "  return output\n",
        "\n",
        "# @jax.jit\n",
        "def encoder_block(inputs, filters, kernel_size):\n",
        "  \"\"\"Defines an encoder block with downsampling and skip connection (jitted).\"\"\"\n",
        "  # ... apply two convolutional blocks\n",
        "  conv = conv_block(inputs, filters, kernel_size)\n",
        "  skip_connection = conv\n",
        "\n",
        "  down_sampled = nn.max_pool(conv, window_shape=(2, 2), strides=(2, 2), padding='SAME')  # apply max pooling for downsampling\n",
        "\n",
        "  return down_sampled, skip_connection\n",
        "\n",
        "# @jax.jit\n",
        "def decoder_block(inputs, skip_connection, filters, kernel_size):\n",
        "  \"\"\"Defines a decoder block with upsampling and skip connection (jitted).\"\"\"\n",
        "  # Apply transposed convolution for upsampling\n",
        "  t_conv = nn.ConvTranspose(features=filters, kernel_size=(kernel_size, kernel_size), strides=(2, 2), padding='SAME')(inputs)\n",
        "  print(f\"Shape of t_conv {t_conv.shape}\")\n",
        "  # Concatenate with skip connection from encoder\n",
        "  concatenated = jnp.concatenate([t_conv, skip_connection], axis=-1)\n",
        "  \n",
        "  # Apply two conv blocks and return\n",
        "  out = conv_block(concatenated, filters, kernel_size)\n",
        "  outputs = conv_block(out, filters, kernel_size)\n",
        "\n",
        "  return outputs\n",
        "\n",
        "# @jax.jit\n",
        "def unet(inputs, filters, kernel_size):\n",
        "  \"\"\"Defines the U-Net architecture (jitted).\"\"\"\n",
        "  # ... create encoder blocks with increasing filter depth\n",
        "  # Encoder\n",
        "  enc1, skip1 = encoder_block(inputs, filters, kernel_size)\n",
        "  print(f\"Shape of skip1 {skip1.shape}\")\n",
        "  print(f\"Shape of enc1 {enc1.shape}\")\n",
        "  enc2, skip2 = encoder_block(enc1, filters*2, kernel_size)\n",
        "  print(f\"Shape of skip2 {skip2.shape}\")\n",
        "  print(f\"Shape of enc3 {enc2.shape}\")\n",
        "  enc3, skip3 = encoder_block(enc2, filters*4, kernel_size)\n",
        "  print(f\"Shape of skip3 {skip3.shape}\")\n",
        "  print(f\"Shape of enc3 {enc3.shape}\")\n",
        "  enc4, skip4 = encoder_block(enc3, filters*8, kernel_size)\n",
        "  print(f\"Shape of skip4 {skip4.shape}\")\n",
        "  print(f\"Shape of enc4 {enc4.shape}\")\n",
        "\n",
        "  # Bottom\n",
        "  bottom = conv_block(enc4, filters*8, kernel_size)\n",
        "\n",
        "  print(f\"Shape of bottom {bottom.shape}\")\n",
        "\n",
        "  # ... create decoder blocks with decreasing filter depth\n",
        "  # Decoder\n",
        "  dec1 = decoder_block(bottom, skip4, filters*8, kernel_size)\n",
        "  dec2 = decoder_block(dec1, skip3, filters*4, kernel_size)\n",
        "  dec3 = decoder_block(dec2, skip2, filters*2, kernel_size)\n",
        "  dec4 = decoder_block(dec3, skip1, filters, kernel_size)\n",
        "\n",
        "  # ... apply final convolution for regression output (e.g., 4 channels for tumor segmentation)\n",
        "  outputs = conv_layer(dec4, 4, 1)\n",
        "\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuLBD8Yu8fz1"
      },
      "source": [
        "**3. Data Acquisition and Preprocessing:**\n",
        "\n",
        "* Download the BraTS 2020 dataset from [https://www.kaggle.com/datasets/mateuszbuda/lgg-mri-segmentation](https://www.kaggle.com/datasets/mateuszbuda/lgg-mri-segmentation).\n",
        "* Preprocess the data by:\n",
        "    * Resizing images to a fixed size suitable for your model.\n",
        "    * Normalizing pixel intensities (e.g., scaling between 0 and 1).\n",
        "    * Segmenting the brain region using provided masks if necessary.\n",
        "    * Splitting the data into training, validation, and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "P_2LGV4u_610"
      },
      "outputs": [],
      "source": [
        "def load_data(image_folder):\n",
        "    image_paths = sorted([os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith('.tif')])\n",
        "\n",
        "    images = [cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) for img_path in image_paths]\n",
        "\n",
        "    return jnp.array(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "8ykG256G_-YV"
      },
      "outputs": [],
      "source": [
        "def preprocess_images(images):\n",
        "\n",
        "    images = np.array(images)\n",
        "\n",
        "    print(f\"Images shape: {images[0].shape}\")\n",
        "  \n",
        "    # Resize images to a fixed size suitable for your model\n",
        "    resized_images = [cv2.resize(img, (256, 256)) for img in images]\n",
        "    # Normalize pixel intensities (scaling between 0 and 1)\n",
        "    normalized_images = [img / 255.0 for img in resized_images]\n",
        "    return jnp.array(normalized_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {
        "id": "G7wBwoRN8fz2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images shape: (256, 256)\n",
            "Images shape: (256, 256)\n",
            "Train images size: (2514, 256, 256)\n",
            "Validation images size: (629, 256, 256)\n",
            "Test images size: (786, 256, 256)\n",
            "Train masks size: (2514, 256, 256)\n",
            "Validation masks size: (629, 256, 256)\n",
            "Test masks size: (786, 256, 256)\n",
            "Image size: (256, 256)\n",
            "Mask size: (256, 256)\n"
          ]
        }
      ],
      "source": [
        "# Load your image segmentation dataset (modify accordingly)\n",
        "# ...\n",
        "image_folder = \"Brain_Tumour/Dataset/images\"\n",
        "mask_folder = \"Brain_Tumour/Dataset/masks\"\n",
        "\n",
        "# Load data\n",
        "images = load_data(image_folder)\n",
        "masks = load_data(mask_folder)\n",
        "\n",
        "# Preprocess data\n",
        "processed_images = preprocess_images(images)\n",
        "processed_masks = preprocess_images(masks)\n",
        "\n",
        "# Preprocess data (normalize and resize). Then split into train, test and validation:\n",
        "\n",
        "# Split into train and validation:\n",
        "train_images, test_images, train_masks, test_masks = train_test_split(processed_images,\n",
        "                                                                    processed_masks,\n",
        "                                                                    test_size=0.2,\n",
        "                                                                    random_state=42)\n",
        "\n",
        "# If you want to further split the training set into training and testing sets, you can do so like this:\n",
        "train_images, val_images, train_masks, val_masks = train_test_split(train_images,\n",
        "                                                                      train_masks,\n",
        "                                                                      test_size=0.2,\n",
        "                                                                      random_state=42)\n",
        "\n",
        "# Print sizes of train and test sets\n",
        "print(f\"Train images size: {train_images.shape}\")\n",
        "print(f\"Validation images size: {val_images.shape}\")\n",
        "print(f\"Test images size: {test_images.shape}\")\n",
        "print(f\"Train masks size: {train_masks.shape}\")\n",
        "print(f\"Validation masks size: {val_masks.shape}\")\n",
        "print(f\"Test masks size: {test_masks.shape}\")\n",
        "print(f\"Image size: {test_images[0].shape}\")\n",
        "print(f\"Mask size: {test_masks[0].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cmWwtPA8fz2"
      },
      "source": [
        "**4. Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    filters: int\n",
        "    kernel_size: int\n",
        "    unet: Callable\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.unet = unet\n",
        "        super().__post_init__()\n",
        "    \n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        return self.unet(x, self.filters, self.kernel_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv1(256, 256, 64)\n",
            "conv2(256, 256, 64)\n",
            "Shape of skip1 (256, 256, 64)\n",
            "Shape of enc1 (128, 128, 64)\n",
            "conv1(128, 128, 128)\n",
            "conv2(128, 128, 128)\n",
            "Shape of skip2 (128, 128, 128)\n",
            "Shape of enc3 (64, 64, 128)\n",
            "conv1(64, 64, 256)\n",
            "conv2(64, 64, 256)\n",
            "Shape of skip3 (64, 64, 256)\n",
            "Shape of enc3 (32, 32, 256)\n",
            "conv1(32, 32, 512)\n",
            "conv2(32, 32, 512)\n",
            "Shape of skip4 (32, 32, 512)\n",
            "Shape of enc4 (16, 16, 512)\n",
            "conv1(16, 16, 512)\n",
            "conv2(16, 16, 512)\n",
            "Shape of bottom (16, 16, 512)\n",
            "Shape of t_conv (32, 32, 512)\n",
            "conv1(32, 32, 512)\n",
            "conv2(32, 32, 512)\n",
            "conv1(32, 32, 512)\n",
            "conv2(32, 32, 512)\n",
            "Shape of t_conv (64, 64, 256)\n",
            "conv1(64, 64, 256)\n",
            "conv2(64, 64, 256)\n",
            "conv1(64, 64, 256)\n",
            "conv2(64, 64, 256)\n",
            "Shape of t_conv (128, 128, 128)\n",
            "conv1(128, 128, 128)\n",
            "conv2(128, 128, 128)\n",
            "conv1(128, 128, 128)\n",
            "conv2(128, 128, 128)\n",
            "Shape of t_conv (256, 256, 64)\n",
            "conv1(256, 256, 64)\n",
            "conv2(256, 256, 64)\n",
            "conv1(256, 256, 64)\n",
            "conv2(256, 256, 64)\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "epochs = 50\n",
        "\n",
        "# Initialize model parameters\n",
        "key = random.PRNGKey(0)\n",
        "\n",
        "input_shape = 256\n",
        "\n",
        "# Define filters and kernel_size\n",
        "filters = 64\n",
        "kernel_size = 5\n",
        "\n",
        "# Initialize the model\n",
        "unet_model = UNet(filters=filters, kernel_size=kernel_size, unet=unet)\n",
        "params = unet_model.init(key, jnp.zeros((input_shape, input_shape, 1), jnp.float32))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(params, images, masks):\n",
        "    logits = unet_model.apply(params, images)\n",
        "    return jnp.mean((logits - masks)**2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the training step function\n",
        "# @jax.jit\n",
        "def train_step(params, images, masks):\n",
        "    grad_fn = jax.grad(loss_fn)\n",
        "    grads = grad_fn(params, images, masks)\n",
        "    updated_params = {key: params[key] - learning_rate * grads[key] for key in params}\n",
        "    return updated_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {
        "id": "FEg0A1UZ8fz2"
      },
      "outputs": [
        {
          "ename": "ScopeParamShapeError",
          "evalue": "Initializer expected to generate shape (5, 5, 1, 64) but got shape (5, 5, 256, 64) instead for parameter \"kernel\" in \"/Conv_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[328], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m batch_images \u001b[38;5;241m=\u001b[39m train_images[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m32\u001b[39m]\n\u001b[1;32m      6\u001b[0m batch_masks \u001b[38;5;241m=\u001b[39m train_masks[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m32\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     10\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m loss_fn(params, batch_images, batch_masks)\n",
            "Cell \u001b[0;32mIn[312], line 5\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(params, images, masks)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(params, images, masks):\n\u001b[1;32m      4\u001b[0m     grad_fn \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mgrad(loss_fn)\n\u001b[0;32m----> 5\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     updated_params \u001b[38;5;241m=\u001b[39m {key: params[key] \u001b[38;5;241m-\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m grads[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m params}\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m updated_params\n",
            "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
            "Cell \u001b[0;32mIn[311], line 2\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params, images, masks)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(params, images, masks):\n\u001b[0;32m----> 2\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43munet_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmean((logits \u001b[38;5;241m-\u001b[39m masks)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
            "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
            "Cell \u001b[0;32mIn[304], line 14\u001b[0m, in \u001b[0;36mUNet.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[302], line 70\u001b[0m, in \u001b[0;36munet\u001b[0;34m(inputs, filters, kernel_size)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Defines the U-Net architecture (jitted).\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# ... create encoder blocks with increasing filter depth\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m enc1, skip1 \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of skip1 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mskip1\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of enc1 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menc1\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[302], line 43\u001b[0m, in \u001b[0;36mencoder_block\u001b[0;34m(inputs, filters, kernel_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Defines an encoder block with downsampling and skip connection (jitted).\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# ... apply two convolutional blocks\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m conv \u001b[38;5;241m=\u001b[39m \u001b[43mconv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m skip_connection \u001b[38;5;241m=\u001b[39m conv\n\u001b[1;32m     46\u001b[0m down_sampled \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mmax_pool(conv, window_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), strides\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAME\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# apply max pooling for downsampling\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[302], line 31\u001b[0m, in \u001b[0;36mconv_block\u001b[0;34m(inputs, filters, kernel_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Defines a convolutional block with activation and normalization (jitted).\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# ... implement convolutional layers, ReLU activation, and batch normalization\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m conv1 \u001b[38;5;241m=\u001b[39m \u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv1\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconv1\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m conv2 \u001b[38;5;241m=\u001b[39m conv_layer(conv1, filters, kernel_size)\n",
            "Cell \u001b[0;32mIn[302], line 16\u001b[0m, in \u001b[0;36mconv_layer\u001b[0;34m(inputs, filters, kernel_size, padding)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mDefines a single convolutional layer with ReLU activation and batch normalization.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m  Output tensor after applying convolution, ReLU activation, and batch normalization.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Apply convolution\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m conv \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Apply ReLU activation (optional)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m activated \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mrelu(conv)\n",
            "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/envs/py3.9/lib/python3.9/site-packages/flax/linen/linear.py:638\u001b[0m, in \u001b[0;36m_Conv.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m kernel_shape:\n\u001b[1;32m    633\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMask needs to have the same shape as weights. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShapes are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    636\u001b[0m   )\n\u001b[0;32m--> 638\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkernel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    643\u001b[0m   kernel \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/envs/py3.9/lib/python3.9/site-packages/flax/core/scope.py:982\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[0m\n\u001b[1;32m    977\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m val, abs_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(value_flat, abs_value_flat):\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# NOTE: We could check dtype consistency here as well but it's\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;66;03m# usefuleness is less obvious. We might intentionally change the dtype\u001b[39;00m\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;66;03m# for inference to a half float type for example.\u001b[39;00m\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mshape(val) \u001b[38;5;241m!=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mshape(abs_val):\n\u001b[0;32m--> 982\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamShapeError(\n\u001b[1;32m    983\u001b[0m         name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text, jnp\u001b[38;5;241m.\u001b[39mshape(abs_val), jnp\u001b[38;5;241m.\u001b[39mshape(val)\n\u001b[1;32m    984\u001b[0m       )\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "\u001b[0;31mScopeParamShapeError\u001b[0m: Initializer expected to generate shape (5, 5, 1, 64) but got shape (5, 5, 256, 64) instead for parameter \"kernel\" in \"/Conv_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "rng = jax.random.PRNGKey(0)\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(train_images), 32):\n",
        "        batch_images = train_images[i:i+32]\n",
        "        batch_masks = train_masks[i:i+32]\n",
        "\n",
        "        params = train_step(params, batch_images, batch_masks)\n",
        "        if i % 100 == 0:\n",
        "            loss_value = loss_fn(params, batch_images, batch_masks)\n",
        "            print(f\"Epoch {epoch}, Step {i}, Loss: {loss_value}\")\n",
        "\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTgGCmGq8fz3"
      },
      "source": [
        "**5. Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LISqRLzZod-S"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def dice_coefficient(pred, target):\n",
        "    \"\"\"Compute Dice coefficient.\"\"\"\n",
        "    intersection = jnp.sum(pred * target)\n",
        "    union = jnp.sum(pred) + jnp.sum(target)\n",
        "    return (2. * intersection) / (union + 1e-7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def jaccard_index(pred, target):\n",
        "    \"\"\"Compute Jaccard index.\"\"\"\n",
        "    intersection = jnp.sum(pred * target)\n",
        "    union = jnp.sum(pred) + jnp.sum(target) - intersection\n",
        "    return (intersection) / (union + 1e-7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def evaluate(params, images, masks):\n",
        "  \"\"\"Evaluation with jitted metric calculation (jitted).\"\"\"\n",
        "  logits = unet_model.apply(params, None, images)\n",
        "\n",
        "  # Convert logits to binary predictions\n",
        "  predictions = jnp.argmax(logits, axis=-1)\n",
        "  predictions = jax.nn.one_hot(predictions, 4)\n",
        "\n",
        "  # Compute metrics for each segmentation channel\n",
        "  dice_scores = jnp.array([dice_coefficient(predictions[:, :, :, i], masks[:, :, :, i]) for i in range(4)])\n",
        "  jaccard_scores = jnp.array([jaccard_index(predictions[:, :, :, i], masks[:, :, :, i]) for i in range(4)])\n",
        "\n",
        "  return {\"Dice\": dice_scores, \"Jaccard\": jaccard_scores}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBLa5R2t8fz3"
      },
      "outputs": [],
      "source": [
        "val_metrics = evaluate(params, val_images, val_masks)\n",
        "print(f\"Validation metrics: {val_metrics}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization function\n",
        "def visualize_results(images, masks, predictions, num_samples=5):\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 15))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        axes[i, 0].imshow(images[i].squeeze(), cmap='gray')\n",
        "        axes[i, 0].set_title(\"Input Image\")\n",
        "        axes[i, 0].axis(\"off\")\n",
        "\n",
        "        axes[i, 1].imshow(masks[i].squeeze(), cmap='gray')\n",
        "        axes[i, 1].set_title(\"True Mask\")\n",
        "        axes[i, 1].axis(\"off\")\n",
        "\n",
        "        axes[i, 2].imshow(predictions[i].squeeze(), cmap='gray')\n",
        "        axes[i, 2].set_title(\"Predicted Mask\")\n",
        "        axes[i, 2].axis(\"off\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming params and test_images are defined\n",
        "test_logits = unet_model.apply(params, None, test_images)\n",
        "test_predictions = jnp.argmax(test_logits, axis=-1)\n",
        "visualize_results(test_images[:5], test_masks[:5], test_predictions[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA7_50cJ8fz3"
      },
      "source": [
        "**6. Summary and Next Steps**\n",
        "\n",
        "This notebook provides a foundation for building and training a U-Net model with JAX for image segmentation while emphasizing the importance of `jax.jit` for performance optimization. Remember to:\n",
        "\n",
        "* Replace the placeholders in the code with appropriate JAX operations and functions based on your chosen dataset and architecture details.\n",
        "* Experiment with different hyperparameters (learning rate, filters, etc.) and training strategies to improve the model's performance.\n",
        "* Explore advanced techniques like data augmentation and regularization to enhance modelgeneralizability and robustness.\n",
        "\n",
        "By completing this notebook and understanding the U-Net architecture, you can gain valuable practical experience in building and training deep learning models for image segmentation tasks using JAX effectively."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
